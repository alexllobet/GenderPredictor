{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 11:39:58.357922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-17 11:39:58.839612: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-17 11:39:58.952027: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-17 11:39:58.952069: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-17 11:39:59.026782: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-17 11:40:01.214889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-17 11:40:01.215092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-17 11:40:01.215108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from os import walk\n",
    "import pandas as pd, numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = []\n",
    "data_path = './data'\n",
    "for (dirpath, dirnames, filenames) in walk(data_path):\n",
    "    data_files.extend(filenames)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2052781, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Name', 'Gender', 'Count'])\n",
    "for file in data_files:\n",
    "    df = pd.concat([df, pd.read_csv(f'{data_path}/{file}', names=['Name', 'Gender','Count'])], ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name Gender  Count\n",
      "0    Ashley      F  38457\n",
      "1   Jessica      F  38358\n",
      "2    Amanda      F  25035\n",
      "3  Brittany      F  24982\n",
      "4     Sarah      F  24635\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding more data\n",
    "df_ext = pd.read_csv('name_gender_dataset.csv').drop(['Probability'], axis=1)\n",
    "names_set_ext = set(df_ext.Name.values)\n",
    "names_set = set(df_prob.Name.values)\n",
    "print(len(names_set_ext - names_set)/len(names_set)*100, '% of increment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_ext.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df, df_ext], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842915\n",
      "1209866\n"
     ]
    }
   ],
   "source": [
    "total_men = df.loc[df.Gender=='M'].shape[0]\n",
    "total_women = df.loc[df.Gender=='F'].shape[0]\n",
    "print(total_men)\n",
    "print(total_women)\n",
    "df_prob = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count feature is not very useful *per se*. Change to observed probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_prob.loc[df.Gender=='M', 'Count'] = df.loc[df.Gender=='M'].Count.apply(lambda count: 100*count/total_men)\n",
    "df_prob.loc[df.Gender=='F', 'Count'] = df.loc[df.Gender=='F'].Count.apply(lambda count: 100*count/total_women)\n",
    "df_prob.rename({\"Count\":\"Prob\"}, inplace=True, axis='columns')\n",
    "print(df_prob.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. Whether there are names that are both male and female. If so, get the one with greatest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shared_names = set(df_prob.loc[df_prob.Gender=='F'].Name.values).intersection(set(df_prob.loc[df_prob.Gender=='M'].Name.values))\n",
    "print(df_prob.Name.value_counts())\n",
    "print(df_prob.shape)\n",
    "print(len(shared_names))\n",
    "if len(shared_names) > 0:\n",
    "    df_prob = df_prob.sort_values('Prob', ascending=False).drop_duplicates(subset='Name', keep='first').sort_index()\n",
    "print(df_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob.Name = df_prob.Name.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.860532283782959 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# given a list of names (Corpus), create a char-to-int dictionary\n",
    "def get_dict(corpus:list) -> dict:\n",
    "    char_dict = dict()\n",
    "    pos = 0\n",
    "    for name in corpus:\n",
    "        chars = set(name.lower())\n",
    "        for char in chars:\n",
    "            if char not in char_dict:\n",
    "                #print(name)\n",
    "                char_dict[char] = pos\n",
    "                pos += 1\n",
    "    return char_dict, {v:k for k,v in char_dict.items()}\n",
    "startTime = time.time()\n",
    "char_dict, _ = get_dict(df_prob.Name.values)\n",
    "endTime = time.time() \n",
    "howMuchTime = endTime - startTime\n",
    "print(str(howMuchTime) + \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 0, 'l': 1, 'e': 2, 'y': 3, 'h': 4, 'a': 5, 'j': 6, 'i': 7, 'c': 8, 'd': 9, 'n': 10, 'm': 11, 'b': 12, 'r': 13, 't': 14, 'p': 15, 'z': 16, 'g': 17, 'f': 18, 'u': 19, 'k': 20, 'o': 21, 'v': 22, 'x': 23, 'q': 24, 'w': 25}\n"
     ]
    }
   ],
   "source": [
    "len(char_dict)\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus of 12661428 words, vocab reduced to 26.\n",
      "0.20936822891235352 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_dict_2(corpus:list) -> (dict,dict):\n",
    "\n",
    "    raw_text = ''.join(corpus)   #periods have not been removed for better results\n",
    "\n",
    "    # creates mapping of unique characters to integers\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))# Prints the total characters and character vocab size\n",
    "    print(f'Corpus of {len(raw_text)} words, vocab reduced to {len(chars)}.')\n",
    "    return char_to_int, int_to_char\n",
    "          \n",
    "\n",
    "startTime = time.time()\n",
    "char_dict,_ = get_dict_2(df_prob.Name.values)\n",
    "endTime = time.time() \n",
    "howMuchTime = endTime - startTime\n",
    "print(str(howMuchTime) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    df_grouped = df.groupby(by=['Name']).sum().rename({'Count':'Total'}, axis=1)\n",
    "    df_probability = df.join(df_grouped.drop(['Gender'],axis=1), on='Name')\n",
    "    df_probability['Prob'] = df_probability.Count/df_probability.Total\n",
    "    return df_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Name Gender  Count\n",
      "0        ashley      F  38457\n",
      "16418    ashley      M    168\n",
      "28127    ashley      F     15\n",
      "32633    ashley      M     50\n",
      "42282    ashley      M     39\n",
      "...         ...    ...    ...\n",
      "1995909  ashley      F  26601\n",
      "2012722  ashley      M    112\n",
      "2028322  ashley      M     30\n",
      "2031371  ashley      F  54855\n",
      "2045066  ashley      M    409\n",
      "\n",
      "[224 rows x 3 columns]\n",
      "              Name Gender Count  Total Prob\n",
      "5514      caleesha      F    17     17  1.0\n",
      "6537       miyisha      F    14     14  1.0\n",
      "7452        silken      F    12     12  1.0\n",
      "7600       cashala      F    11     11  1.0\n",
      "8643     shaunique      F    10     10  1.0\n",
      "...            ...    ...   ...    ...  ...\n",
      "2052656    termell      M     5      5  1.0\n",
      "2052698     tryell      M     5      5  1.0\n",
      "2052710     undrae      M     5      5  1.0\n",
      "2052740  willfredo      M     5      5  1.0\n",
      "2052760       yler      M     5      5  1.0\n",
      "\n",
      "[21556 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_new = get_probs(df_prob)\n",
    "print(df_prob.loc[df_prob.Name=='ashley'])\n",
    "print(df_new.loc[df_new.Prob==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = df_new.sample(frac=1).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2052781\n"
     ]
    }
   ],
   "source": [
    "list_of_names = list(df_shuffled.Name.values)\n",
    "max_seq = 30\n",
    "def encode_name(name):\n",
    "    encoded = list()\n",
    "    for char in name:\n",
    "        encoded.append(char_dict[char])\n",
    "        if len(encoded) == 30:\n",
    "            return [int(''.join(map(str, encoded)))]\n",
    "    if len(encoded) < 30:\n",
    "        encoded = encoded + (max_seq-len(encoded))*[0]\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "X = np.array([np.array(encode_name(name)) for name in list_of_names]) # char-encode each name \n",
    "print(len(X))\n",
    "# reshapes X to be [samples, time steps, features]\n",
    "X = np.reshape(X, (len(X), max_seq, 1))\n",
    "#weights = df_new.Prob.values\n",
    "weights = df_new.Count.values\n",
    "y = [(1, weight) if gender=='M' else (0, weight) for gender, weight in zip(df_shuffled.Gender.values, weights)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2052781 2052781\n",
      "(2052781, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(X),\n",
    "     len(y))\n",
    "print(df_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=False) # dont shuffle otherwise weights messed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 38457), (1, 38358), (0, 25035)]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_train = np.array([packed[1] for packed in y_train])\n",
    "weights_test = [packed[1] for packed in y_test]\n",
    "y_train = np.array([packed[0] for packed in y_train])\n",
    "y_test = [packed[0] for packed in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0]\n",
      "[38457 38358 25035 24982]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:4])\n",
    "print(weights_train[:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 14:58:44.002613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-14 14:58:44.002656: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-14 14:58:44.002676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-10-14 14:58:44.004259: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 192)           4992      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 192)               295680    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 193       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300,865\n",
      "Trainable params: 300,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim =192\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_dict), embedding_dim, input_length=max_seq))\n",
    "model.add(LSTM(192))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam' , metrics=\"acc\", weighted_metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11587/11587 [==============================] - 1402s 121ms/step - loss: 105.7546 - acc: 0.6934 - weighted_acc: 0.6934 - val_loss: 103.8424 - val_acc: 0.7139 - val_weighted_acc: 0.7120\n",
      "Epoch 2/30\n",
      "11587/11587 [==============================] - 1463s 126ms/step - loss: 95.7274 - acc: 0.7265 - weighted_acc: 0.7358 - val_loss: 98.4403 - val_acc: 0.7400 - val_weighted_acc: 0.7370\n",
      "Epoch 3/30\n",
      "11587/11587 [==============================] - 1500s 129ms/step - loss: 88.9422 - acc: 0.7456 - weighted_acc: 0.7605 - val_loss: 93.7888 - val_acc: 0.7525 - val_weighted_acc: 0.7544\n",
      "Epoch 4/30\n",
      "11587/11587 [==============================] - 1595s 138ms/step - loss: 82.3983 - acc: 0.7598 - weighted_acc: 0.7847 - val_loss: 93.0810 - val_acc: 0.7640 - val_weighted_acc: 0.7605\n",
      "Epoch 5/30\n",
      "11587/11587 [==============================] - 166269s 14s/step - loss: 76.9430 - acc: 0.7689 - weighted_acc: 0.8032 - val_loss: 88.5305 - val_acc: 0.7774 - val_weighted_acc: 0.7715\n",
      "Epoch 6/30\n",
      "11587/11587 [==============================] - 1486s 128ms/step - loss: 72.3551 - acc: 0.7763 - weighted_acc: 0.8196 - val_loss: 87.1830 - val_acc: 0.7782 - val_weighted_acc: 0.7798\n",
      "Epoch 7/30\n",
      "11587/11587 [==============================] - 1248s 108ms/step - loss: 67.9930 - acc: 0.7828 - weighted_acc: 0.8337 - val_loss: 86.4460 - val_acc: 0.7851 - val_weighted_acc: 0.7852\n",
      "Epoch 8/30\n",
      "11587/11587 [==============================] - 1424s 123ms/step - loss: 65.1367 - acc: 0.7883 - weighted_acc: 0.8416 - val_loss: 87.9415 - val_acc: 0.7871 - val_weighted_acc: 0.7895\n",
      "Epoch 9/30\n",
      "11587/11587 [==============================] - 1264s 109ms/step - loss: 62.7497 - acc: 0.7908 - weighted_acc: 0.8488 - val_loss: 84.0868 - val_acc: 0.7881 - val_weighted_acc: 0.7903\n",
      "Epoch 10/30\n",
      "11587/11587 [==============================] - 1319s 114ms/step - loss: 60.4955 - acc: 0.7936 - weighted_acc: 0.8549 - val_loss: 83.9022 - val_acc: 0.7921 - val_weighted_acc: 0.7923\n",
      "Epoch 11/30\n",
      "11587/11587 [==============================] - 1210s 104ms/step - loss: 58.5230 - acc: 0.7974 - weighted_acc: 0.8611 - val_loss: 84.5535 - val_acc: 0.7960 - val_weighted_acc: 0.7969\n",
      "Epoch 12/30\n",
      "11587/11587 [==============================] - 1218s 105ms/step - loss: 57.0795 - acc: 0.7988 - weighted_acc: 0.8648 - val_loss: 83.8666 - val_acc: 0.7956 - val_weighted_acc: 0.7973\n",
      "Epoch 13/30\n",
      "11587/11587 [==============================] - 1202s 104ms/step - loss: 55.6351 - acc: 0.8012 - weighted_acc: 0.8668 - val_loss: 82.5740 - val_acc: 0.7980 - val_weighted_acc: 0.7995\n",
      "Epoch 14/30\n",
      "11587/11587 [==============================] - 1196s 103ms/step - loss: 55.1854 - acc: 0.8018 - weighted_acc: 0.8695 - val_loss: 82.3172 - val_acc: 0.8005 - val_weighted_acc: 0.7998\n",
      "Epoch 15/30\n",
      "11587/11587 [==============================] - 1189s 103ms/step - loss: 53.6232 - acc: 0.8042 - weighted_acc: 0.8721 - val_loss: 81.9559 - val_acc: 0.7995 - val_weighted_acc: 0.7982\n",
      "Epoch 16/30\n",
      "11587/11587 [==============================] - 1195s 103ms/step - loss: 52.6753 - acc: 0.8059 - weighted_acc: 0.8755 - val_loss: 82.3714 - val_acc: 0.8015 - val_weighted_acc: 0.8023\n",
      "Epoch 17/30\n",
      "11587/11587 [==============================] - 1207s 104ms/step - loss: 52.2241 - acc: 0.8062 - weighted_acc: 0.8764 - val_loss: 81.0446 - val_acc: 0.7992 - val_weighted_acc: 0.8029\n",
      "Epoch 18/30\n",
      "11587/11587 [==============================] - 1188s 103ms/step - loss: 51.4966 - acc: 0.8064 - weighted_acc: 0.8784 - val_loss: 82.0543 - val_acc: 0.7979 - val_weighted_acc: 0.8036\n",
      "Epoch 19/30\n",
      "11587/11587 [==============================] - 1194s 103ms/step - loss: 51.1807 - acc: 0.8076 - weighted_acc: 0.8790 - val_loss: 83.3431 - val_acc: 0.8019 - val_weighted_acc: 0.7960\n",
      "Epoch 20/30\n",
      "11587/11587 [==============================] - 1194s 103ms/step - loss: 50.9190 - acc: 0.8084 - weighted_acc: 0.8813 - val_loss: 82.5382 - val_acc: 0.8060 - val_weighted_acc: 0.7998\n",
      "Epoch 20: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e720e8640>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3)\n",
    "model.fit(np.array(X_train), np.array(y_train),  validation_split=0.15, epochs = 30, sample_weight=weights_train, batch_size=128, callbacks=[earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2406/2406 [==============================] - 100s 41ms/step - loss: 0.4351 - acc: 0.8065 - weighted_acc: 0.8065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4350805878639221, 0.8064906597137451, 0.8064906597137451]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(X_test), np.array(y_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
