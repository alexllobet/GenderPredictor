{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 13:54:11.782687: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 13:54:12.312245: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-10 13:54:12.391875: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-10 13:54:12.391918: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-10 13:54:12.460921: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-10 13:54:14.165541: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 13:54:14.166305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 13:54:14.166349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from os import walk\n",
    "import pandas as pd, numpy as np\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = []\n",
    "data_path = './data'\n",
    "for (dirpath, dirnames, filenames) in walk(data_path):\n",
    "    data_files.extend(filenames)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2052781, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Name', 'Gender', 'Count'])\n",
    "for file in data_files:\n",
    "    df = pd.concat([df, pd.read_csv(f'{data_path}/{file}', names=['Name', 'Gender','Count'])], ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name Gender  Count\n",
      "0    Ashley      F  38457\n",
      "1   Jessica      F  38358\n",
      "2    Amanda      F  25035\n",
      "3  Brittany      F  24982\n",
      "4     Sarah      F  24635\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842915\n",
      "1209866\n"
     ]
    }
   ],
   "source": [
    "total_men = df.loc[df.Gender=='M'].shape[0]\n",
    "total_women = df.loc[df.Gender=='F'].shape[0]\n",
    "print(total_men)\n",
    "print(total_women)\n",
    "df_prob = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count feature is not very useful *per se*. Change to observed probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name Gender      Prob\n",
      "0    Ashley      F  3.178616\n",
      "1   Jessica      F  3.170434\n",
      "2    Amanda      F  2.069237\n",
      "3  Brittany      F  2.064857\n",
      "4     Sarah      F  2.036176\n"
     ]
    }
   ],
   "source": [
    "df_prob.loc[df.Gender=='M', 'Count'] = df.loc[df.Gender=='M'].Count.apply(lambda count: 100*count/total_men)\n",
    "df_prob.loc[df.Gender=='F', 'Count'] = df.loc[df.Gender=='F'].Count.apply(lambda count: 100*count/total_women)\n",
    "df_prob.rename({\"Count\":\"Prob\"}, inplace=True, axis='columns')\n",
    "print(df_prob.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. Whether there are names that are both male and female. If so, get the one with greatest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marion     284\n",
      "Sidney     284\n",
      "John       284\n",
      "James      284\n",
      "William    284\n",
      "          ... \n",
      "Nevach       1\n",
      "Ndey         1\n",
      "Nashmia      1\n",
      "Naiome       1\n",
      "Yler         1\n",
      "Name: Name, Length: 101338, dtype: int64\n",
      "(2052781, 3)\n",
      "11282\n",
      "(2052781, 3)\n"
     ]
    }
   ],
   "source": [
    "shared_names = set(df_prob.loc[df_prob.Gender=='F'].Name.values).intersection(set(df_prob.loc[df_prob.Gender=='M'].Name.values))\n",
    "print(df_prob.Name.value_counts())\n",
    "print(df_prob.shape)\n",
    "print(len(shared_names))\n",
    "if len(shared_names) > 0:\n",
    "    df_prob = df_prob.sort_values('Prob', ascending=False).drop_duplicates(subset='Name', keep='first').sort_index()\n",
    "print(df_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_477278/3957502242.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df_prob.Names = df_prob.Name.apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "df_prob.Names = df_prob.Name.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.486760139465332 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# given a list of names (Corpus), create a char-to-int dictionary\n",
    "def get_dict(corpus:list) -> dict:\n",
    "    char_dict = dict()\n",
    "    pos = 0\n",
    "    for name in corpus:\n",
    "        chars = set(name.lower())\n",
    "        for char in chars:\n",
    "            if char not in char_dict:\n",
    "                char_dict[char] = pos\n",
    "                pos += 1\n",
    "    return char_dict, {v:k for k,v in char_dict.items()}\n",
    "startTime = time.time()\n",
    "char_dict, _ = get_dict(df_prob.Name.values)\n",
    "endTime = time.time() \n",
    "howMuchTime = endTime - startTime\n",
    "print(str(howMuchTime) + \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'e': 1, 's': 2, 'y': 3, 'h': 4, 'l': 5, 'c': 6, 'i': 7, 'j': 8, 'd': 9, 'm': 10, 'n': 11, 't': 12, 'r': 13, 'b': 14, 'p': 15, 'z': 16, 'g': 17, 'f': 18, 'u': 19, 'k': 20, 'o': 21, 'v': 22, 'x': 23, 'q': 24, 'w': 25}\n"
     ]
    }
   ],
   "source": [
    "len(char_dict)\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus of 12661428 words, vocab reduced to 26.\n",
      "0.20739054679870605 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_dict_2(corpus:list) -> (dict,dict):\n",
    "\n",
    "    raw_text = ''.join(corpus)   #periods have not been removed for better results\n",
    "\n",
    "    # creates mapping of unique characters to integers\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))# Prints the total characters and character vocab size\n",
    "    print(f'Corpus of {len(raw_text)} words, vocab reduced to {len(chars)}.')\n",
    "    return char_to_int, int_to_char\n",
    "          \n",
    "\n",
    "startTime = time.time()\n",
    "char_dict,_ = get_dict_2(df_prob.Names.values)\n",
    "endTime = time.time() \n",
    "howMuchTime = endTime - startTime\n",
    "print(str(howMuchTime) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names = list(df_prob.Names.values)\n",
    "max_seq = 30\n",
    "def encode_name(name):\n",
    "    encoded = list()\n",
    "    for char in name:\n",
    "        encoded.append(char_dict[char])\n",
    "        if len(encoded) == 30:\n",
    "            return [int(''.join(map(str, encoded)))]\n",
    "    if len(encoded) < 30:\n",
    "        encoded = encoded + (max_seq-len(encoded))*[0]\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "X = np.array([np.array(encode_name(name)) for name in list_of_names]) # char-encode each name \n",
    "# reshapes X to be [samples, time steps, features]\n",
    "X = np.reshape(X, (len(X), max_seq, 1))\n",
    "y = [1 if gender=='M' else 0 for gender in df_prob.Gender.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 13:54:39.598347: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-10 13:54:39.598978: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-10 13:54:39.599039: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-10-10 13:54:39.600849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 192)           4992      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 192)               295680    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 193       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300,865\n",
      "Trainable params: 300,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim =192\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_dict), embedding_dim, input_length=max_seq))\n",
    "model.add(LSTM(192))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=\"acc\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11587/11587 [==============================] - 1369s 118ms/step - loss: 0.4292 - acc: 0.7941 - val_loss: 0.3389 - val_acc: 0.8429\n",
      "Epoch 2/30\n",
      "11587/11587 [==============================] - 1313s 113ms/step - loss: 0.3136 - acc: 0.8522 - val_loss: 0.2991 - val_acc: 0.8566\n",
      "Epoch 3/30\n",
      "11587/11587 [==============================] - 1470s 127ms/step - loss: 0.2880 - acc: 0.8611 - val_loss: 0.2877 - val_acc: 0.8594\n",
      "Epoch 4/30\n",
      "11587/11587 [==============================] - 1449s 125ms/step - loss: 0.2769 - acc: 0.8646 - val_loss: 0.2799 - val_acc: 0.8637\n",
      "Epoch 5/30\n",
      "11587/11587 [==============================] - 1400s 121ms/step - loss: 0.2705 - acc: 0.8668 - val_loss: 0.2760 - val_acc: 0.8644\n",
      "Epoch 6/30\n",
      "11587/11587 [==============================] - 1669s 144ms/step - loss: 0.2667 - acc: 0.8683 - val_loss: 0.2727 - val_acc: 0.8656\n",
      "Epoch 7/30\n",
      "11587/11587 [==============================] - 1291s 111ms/step - loss: 0.2638 - acc: 0.8692 - val_loss: 0.2727 - val_acc: 0.8648\n",
      "Epoch 8/30\n",
      "11587/11587 [==============================] - 1130s 97ms/step - loss: 0.2621 - acc: 0.8699 - val_loss: 0.2709 - val_acc: 0.8664\n",
      "Epoch 9/30\n",
      "11587/11587 [==============================] - 1194s 103ms/step - loss: 0.2606 - acc: 0.8706 - val_loss: 0.2685 - val_acc: 0.8667\n",
      "Epoch 10/30\n",
      "11587/11587 [==============================] - 1197s 103ms/step - loss: 0.2594 - acc: 0.8707 - val_loss: 0.2686 - val_acc: 0.8679\n",
      "Epoch 11/30\n",
      "11587/11587 [==============================] - 1266s 109ms/step - loss: 0.2585 - acc: 0.8710 - val_loss: 0.2703 - val_acc: 0.8673\n",
      "Epoch 12/30\n",
      "11587/11587 [==============================] - 1237s 107ms/step - loss: 0.2577 - acc: 0.8713 - val_loss: 0.2676 - val_acc: 0.8676\n",
      "Epoch 13/30\n",
      "11587/11587 [==============================] - 1238s 107ms/step - loss: 0.2573 - acc: 0.8711 - val_loss: 0.2675 - val_acc: 0.8675\n",
      "Epoch 14/30\n",
      "11587/11587 [==============================] - 1260s 109ms/step - loss: 0.2569 - acc: 0.8717 - val_loss: 0.2687 - val_acc: 0.8671\n",
      "Epoch 15/30\n",
      "11587/11587 [==============================] - 1256s 108ms/step - loss: 0.2562 - acc: 0.8716 - val_loss: 0.2678 - val_acc: 0.8675\n",
      "Epoch 16/30\n",
      "11587/11587 [==============================] - 1251s 108ms/step - loss: 0.2562 - acc: 0.8718 - val_loss: 0.2683 - val_acc: 0.8665\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ca4149820>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3)\n",
    "model.fit(np.array(X_train), np.array(y_train), validation_split=0.15, epochs = 30, batch_size=128, callbacks=[earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2406/2406 [==============================] - 102s 42ms/step - loss: 0.2683 - acc: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26834672689437866, 0.8666755557060242]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(X_test), np.array(y_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
